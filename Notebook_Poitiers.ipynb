{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-Poitiers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtjuFZeVnsQaOj7l8Aq89H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavela/Linguistics-with-conllu-data/blob/master/Notebook_Poitiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forewords\n",
        "\n",
        "Focus here on ready-made Python scripts\n",
        "\n",
        "(Although some of the first commands are in Bash)\n",
        "\n",
        "Bash commands need to begin with` ! `here\n",
        "\n",
        "You can also use tagged data with corpus tools, such as Antconc (See https://www.youtube.com/watch?v=gkKna-ka9zw)\n",
        "\n",
        "All the scripts are downloadable at https://github.com/mavela/Linguistics-with-conllu-data.git (disclaimer for code beauty!)"
      ],
      "metadata": {
        "id": "JzWDcp3FOdXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparations\n",
        "Let's download the data from Github - this is done using `git clone + repo address`\n",
        "\n",
        "`Git clones` the entire folder to the colab directory\n",
        "\n",
        "`% cd` takes us to the correct directory\n",
        "\n",
        "`! ls` lists the files in that directory"
      ],
      "metadata": {
        "id": "egn0EwFPO4DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/mavela/Linguistics-with-conllu-data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSZPYN8lPcWK",
        "outputId": "05e04c5d-a16c-4db8-b331-dd52a5ef41dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Linguistics-with-conllu-data'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 63 (delta 0), reused 2 (delta 0), pack-reused 61\u001b[K\n",
            "Unpacking objects: 100% (63/63), done.\n",
            "Checking out files: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd Linguistics-with-conllu-data/\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCHrNsoi4AKp",
        "outputId": "3e3e8f62-ff2d-49d2-c05e-334fdeb28648"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Linguistics-with-conllu-data\n",
            "analyze.py\t   keyness-conllu.py~  stop.txt~\n",
            "analyze.py~\t   keyness-filt.py     text_dispersion_0306.py~\n",
            "common.py\t   keyness-filt.py~    text_dispersion_filt.py\n",
            "data\t\t   keyness.py\t       text_dispersion_filt.py~\n",
            "keyness-conllu.py  __pycache__\t       text_dispersion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Here is the data folder\"\n",
        "! ls data/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zicJiMey4UNz",
        "outputId": "46262f10-2d43-433e-da83-0c0307050d15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the data folder\n",
            "df.conllu  ds.conllu  ht.conllu  sr.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Then let's have a look at the data\n",
        "\n",
        "The files are from the Corpus of Online Register of English (Biber & Egbert 2018)\n",
        "\n",
        "df = discussion forum\n",
        "\n",
        "ds = description with intent to sell\n",
        "\n",
        "sr = sports report\n",
        "\n",
        "ht = how-to\n",
        "\n",
        "You can print N first lines of the file with `head -N`"
      ],
      "metadata": {
        "id": "jYbqyUN7QLXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bpqVTNqG0Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a882ca7-e98f-4871-ebf2-ddcb4695664a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## register SR\n",
            "# ../CORE_raw/sr\n",
            "# ../CORE_raw/sr/1+NA+SR+NA-NA-NA-NA+SR-SR-SR-SR+NNNY+1758585.txt\n",
            "# <1758585>\n",
            "# <http://www.infonews.co.nz/news.cfm?id=89071>\n",
            "# <Rater 1: NA_SR * QU * N * ID: A38IZBOGFD2S9Y>\n",
            "# <Rater 2: NA_SR * QU * N * ID: A3UOK7WMVFVHHE>\n",
            "# <Rater 3: NA_SR *  * N * ID: A33SMNMTMIOJ6T>\n",
            "# <Rater 4: NA_SR *  * Y * ID: A193YDFH1CD4JX>\n",
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = The kiwi crews just failed to qualify for a debut A Final.\n",
            "1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t_\tSpacesBefore=\\n\n",
            "2\tkiwi\tkiwi\tPROPN\tNNP\tNumber=Sing\t3\tcompound\t_\t_\n",
            "3\tcrews\tcrew\tNOUN\tNNS\tNumber=Plur\t5\tnsubj\t_\t_\n",
            "4\tjust\tjust\tADV\tRB\t_\t5\tadvmod\t_\t_\n",
            "5\tfailed\tfail\tVERB\tVBD\tMood=Ind|Tense=Past|VerbForm=Fin\t0\troot\t_\t_\n",
            "6\tto\tto\tPART\tTO\t_\t7\tmark\t_\t_\n",
            "7\tqualify\tqualify\tVERB\tVB\tVerbForm=Inf\t5\txcomp\t_\t_\n"
          ]
        }
      ],
      "source": [
        "! head -20 data/sr.conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First steps with a dataset\n",
        "\n",
        "How many tokens?\n",
        "\n",
        "### Questions:\n",
        "- are there differences between the datasets?"
      ],
      "metadata": {
        "id": "-Qsn46_IR10_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import count_words, most_frequent, extract_register, print_text\n",
        "\n",
        "print(\"Total word count of the conllu file is\", count_words(\"data/sr.conllu\"), \"tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hidsAEzvR_a0",
        "outputId": "b84b356c-ca3e-411c-d957-780b7e552e55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count of the conllu file is 793715 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print words, lemmas or other things\n",
        "\n",
        "`print_text` prints one text per line the text featurized as we indicate.\n",
        "\n",
        "We can specify the kind of feature by referring to the column name in the Conll format.\n",
        "\n",
        "The columns are:\n",
        "\n",
        "`ID, FORM, LEMMA, UPOS, XPOS, FEAT, HEAD, DEPREL, DEPS, MISC`\n",
        "\n",
        "The numeric argument indicates how many texts we want."
      ],
      "metadata": {
        "id": "feb-zWV7TQIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_text(\"data/df.conllu\", \"FORM\", 2))\n",
        "print()\n",
        "## Then try with some other text class and feature!"
      ],
      "metadata": {
        "id": "5jgFn-YWTSGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803ab7e3-56fc-4495-99ef-29b170aec295"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I believe IMHO that the only mobile OS that stood toe to toe with IOS was the WEBOS . It was very polished and felt like IOS on steroids . Yea , app support sucked and Palm 's marketing was the worst thing ever but the WEBOS had potential . I am surprised that Microsoft or Google did n't make an offer to buy it because that could have leveled the playing field . Currently , I have an i Phone 4 and a Palm Pixi Plus . The Palm has webos and it is no where near as good as iOS . Yes , the interface looks good , but the functionality is severely lacking . Maybe , if they had more time , it could have been a competitor , but they did n't and it is n't . I really like the OS as well . It was very advance when compared to other platforms at that time . If they did n't go with Sprint , had a better advertising plan and went with Enyo at the very beginning , I think they would have a chance .\n",
            "Good work sped - hopefully this works like your Crows one did last night PUDDINGS . __________________ Somebody who loves life and thinks that North Melbourne are grouse . A word of warning this is not have anything to do with porn or anything disgusting . So please refrain from posting up pornographic images and posting about it . Also no trolls allowed . __________________ Somebody who loves life and thinks that North Melbourne are grouse . A word of warning this is not have anything to do with porn or anything disgusting . So please refrain from posting up pornographic images and posting about it . Also no trolls allowed .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Type A perspective: Analyzing / searching for individual words / lemmas / something\n",
        "\n",
        "How many times a particular lemma appears in the corpus?\n",
        "\n",
        "What are its most frequent dependency relations? (Or other tags)"
      ],
      "metadata": {
        "id": "jea8SURaVhjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import count_specific_lemma, count_word_context\n",
        "\n",
        "print(count_specific_lemma(\"you\", \"data/sr.conllu\", \"FORM\"))\n",
        "print(count_specific_lemma(\"think\", \"data/sr.conllu\", \"DEPREL\"))"
      ],
      "metadata": {
        "id": "QpO9qD9Z28ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b38fc7-1641-49a4-e7e8-35e017fd4ba0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total counts for the lemma you: 4206\n",
            "The most frequent  FORM:\n",
            "you 2982\n",
            "your 675\n",
            "You 481\n",
            "Your 38\n",
            "YOU 15\n",
            "YOUR 7\n",
            "ur 4\n",
            "youd 1\n",
            "youre 1\n",
            "you're 1\n",
            "UR 1\n",
            "\n",
            "Total counts for the lemma think: 1633\n",
            "The most frequent  DEPREL:\n",
            "root 776\n",
            "conj 233\n",
            "parataxis 149\n",
            "ccomp 137\n",
            "acl:relcl 119\n",
            "advcl 107\n",
            "xcomp 72\n",
            "acl 24\n",
            "csubj 7\n",
            "amod 4\n",
            "appos 3\n",
            "nmod 1\n",
            "obl 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "How does a word appear in context? "
      ],
      "metadata": {
        "id": "v64_XHOSJ6WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import read_conllu\n",
        "ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC=range(10)\n",
        "\n",
        "count = 0\n",
        "for comm, sent in read_conllu(open(\"data/sr.conllu\", \"r\")): # here you can change the data file\n",
        "    if \"you\" in [token[FORM].lower() for token in sent]: #here you can put any word form instead of \"you\"\n",
        "        count += 1\n",
        "        if count > 5:\n",
        "          break\n",
        "        else:\n",
        "          print(\" \".join(token[FORM] for token in sent))\n"
      ],
      "metadata": {
        "id": "dTbK2Qn_UY33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1db40b-107b-4feb-dcb2-586f5716563e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can thank the Chiefs for that , but the Ravens should still be concerned about the breakdowns over the last two weeks .\n",
            "You know what 's funny , compare the Ravens ' situation on defense this year to where it was two years ago under Greg Mattison .\n",
            "W/ the Lewis & Webb injuries -- and you know a forthcoming Reed malady at some pt -- they will lose their share of gms .\n",
            "\" I guess you can say the same on the '96 team .\n",
            "That 's all that matters is trusting in your teammates and knowing you have that ability to get it done .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type B perspective: Text level\n",
        "Most frequent tokens + lemmas in a text"
      ],
      "metadata": {
        "id": "GuM3EzBo3atO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent tokens\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"FORM\", 10)) # the number defines how many we want to see\n",
        "\n",
        "print(\"Most frequent lemmas\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"LEMMA\", 10)) # the number defines how many we want to see"
      ],
      "metadata": {
        "id": "IxqkHsbc3kF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1d9d80-8529-4a55-c934-ea3db185723a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent tokens\n",
            "the 36521\n",
            "to 18835\n",
            "a 16186\n",
            "and 16092\n",
            "of 13359\n",
            "in 12541\n",
            "that 7795\n",
            "is 7462\n",
            "'s 7225\n",
            "I 6943\n",
            "\n",
            "Most frequent lemmas\n",
            "the 39765\n",
            "be 30288\n",
            "to 19114\n",
            "a 18652\n",
            "and 16785\n",
            "he 14093\n",
            "of 13486\n",
            "in 13160\n",
            "have 10916\n",
            "that 8373\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of POS tags or dependency relations\n",
        "\n",
        "The Conllu tagsets (columns) are defined as: `ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC`"
      ],
      "metadata": {
        "id": "c-xnGF5734pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent frequent part-of-speech tags\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"UPOS\", 10))\n",
        "\n",
        "print(\"Then the most frequent dependency relations\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"DEPREL\", 10))"
      ],
      "metadata": {
        "id": "XnBARAuM3787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12815f4-4113-4f65-cb11-663a02c6c56a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent frequent part-of-speech tags\n",
            "NOUN 128413\n",
            "VERB 85405\n",
            "ADP 74483\n",
            "DET 69850\n",
            "PRON 62145\n",
            "PROPN 60028\n",
            "AUX 48437\n",
            "ADJ 47943\n",
            "ADV 43374\n",
            "PART 24079\n",
            "\n",
            "Then the most frequent dependency relations\n",
            "case 75109\n",
            "det 67848\n",
            "nsubj 64654\n",
            "advmod 45371\n",
            "obl 39691\n",
            "obj 37653\n",
            "root 37012\n",
            "amod 35650\n",
            "mark 29506\n",
            "compound 27985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focusing text-level analysis to particular tags"
      ],
      "metadata": {
        "id": "7BH9q1Q14LEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent lemmas under a specific tagset(column).\")\n",
        "print(\"For instance, the most frequent lemmas that receive the ADJ tag in the UPOS column.\")\n",
        "print()\n",
        "print(most_frequent(\"data/sr.conllu\", \"UPOS\", 10, \"ADJ\"))\n",
        "\n",
        "print(\"Or the most frequent lemmas that receive nsubj that in the DEPREL column\")\n",
        "print()\n",
        "print(most_frequent(\"data/sr.conllu\", \"DEPREL\", 10, \"nsubj\"))"
      ],
      "metadata": {
        "id": "krqqnwgK4J_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d61d4c5-36d4-4ed0-d88f-2c5fa228a8d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent lemmas under a specific tagset(column).\n",
            "For instance, the most frequent lemmas that receive the ADJ tag in the UPOS column.\n",
            "\n",
            "last 1561\n",
            "first 1478\n",
            "good 1348\n",
            "more 1077\n",
            "best 872\n",
            "other 858\n",
            "great 716\n",
            "second 714\n",
            "big 618\n",
            "many 613\n",
            "\n",
            "Or the most frequent lemmas that receive nsubj that in the DEPREL column\n",
            "\n",
            "he 7137\n",
            "I 6857\n",
            "it 4280\n",
            "we 4122\n",
            "they 3128\n",
            "you 2900\n",
            "that 2585\n",
            "who 1775\n",
            "this 739\n",
            "which 660\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyness\n",
        "\n",
        "### Standard target+reference corpus comparison with log likelihood\n",
        "\n",
        "`\"FORM\"` specifies the conlllu format column you want to focus, so you can use `\"LEMMA\"` too\n",
        "\n",
        "Note that we call the keyness script from bash here.\n",
        "\n",
        "The keywords are counted for `ds.conllu` in comparison with `ht.conllu`"
      ],
      "metadata": {
        "id": "SYfRXXYl4eNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 keyness-conllu.py data/ds.conllu data/ht.conllu \"FORM\" | head -30 \n"
      ],
      "metadata": {
        "id": "2h-XUOxWGD9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d662da1-41ea-4366-c94d-9d8c386fcf3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Input_Corpus ...\n",
            "done!\n",
            "Reading Reference Corpus ...\n",
            "done!\n",
            "--------------------------------------------------\n",
            "Corpus name Input_Corpus\n",
            "Keyword, keyness value\n",
            "book 2990.88\n",
            "his 1817.9\n",
            "he 1163.28\n",
            "her 933.53\n",
            "of 744.08\n",
            "nov 646.64\n",
            "she 597.91\n",
            "author 578.88\n",
            "was 549.97\n",
            "world 529.82\n",
            "and 495.21\n",
            "cart 454.74\n",
            "amazon 453.0\n",
            "story 430.13\n",
            "price 418.36\n",
            "life 403.67\n",
            "pm 395.34\n",
            "books 378.12\n",
            "album 342.91\n",
            "availability 324.35\n",
            "read 307.57\n",
            "shipping 305.7\n",
            "trial 295.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions: \n",
        "- What happens if you try to get keywords using lemmas? \n",
        "- Do all the registers get good keywords, or are some registers described better?\n",
        "- Does changing the reference corpus change the results?\n",
        "\n",
        "### Note!\n",
        "An essential part of any linguistic analysis is to understand what the numerically extracted results (here the keywords) mean in context. To this end, you  can print texts with the keywords using the `read_conllu.py` above."
      ],
      "metadata": {
        "id": "_54quPa5Gfe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text dispersion keyness\n",
        "\n",
        "Counted the same way as the standard keyness above"
      ],
      "metadata": {
        "id": "Tpv9i-0A_fFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 text_dispersion.py data/sr.conllu data/df.conllu \"FORM\" | head -40"
      ],
      "metadata": {
        "id": "4AHOEtJ5T3Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169f9fd6-246d-40ef-fcc0-f895422c82c8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading a conllu file named data/df.conllu\n",
            "Reading a conllu file named data/sr.conllu\n",
            "\n",
            "Text dispersion keywords for data/sr.conllu\n",
            "season 278.63649189897933\n",
            "team 249.8567972281745\n",
            "league 209.3395896410617\n",
            "win 194.15854527065773\n",
            "coach 182.90443514605633\n",
            "game 164.5988062685827\n",
            "three 163.6876659620223\n",
            "players 161.96862063145053\n",
            "five 159.88954214316306\n",
            "four 149.69814763788727\n",
            "won 147.4713323782174\n",
            "teams 143.81480788711949\n",
            "victory 142.96050040128654\n",
            "games 140.70458967799044\n",
            "against 136.05173611466475\n",
            "scoring 134.52289929812304\n",
            "play 133.02157653240735\n",
            "-- 130.83359240712065\n",
            "saturday 130.80478903072807\n",
            "winning 123.98408080064502\n",
            "football 122.63054198772535\n",
            "his 122.22517218335568\n",
            "scored 120.39621800857634\n",
            "cup 118.81271430035525\n",
            "injury 117.51784083340803\n",
            "final 115.9205114753092\n",
            "player 114.7401192420264\n",
            "played 113.65815584424207\n",
            "champions 113.55789009126187\n",
            "fourth 113.0287886537727\n",
            "seven 112.62354106263122\n",
            "six 111.62604043035991\n",
            "ball 110.1619835408664\n",
            "goal 107.52470002995996\n",
            "second 104.6161183893258\n",
            "points 101.96761693240062\n",
            "Traceback (most recent call last):\n",
            "  File \"text_dispersion.py\", line 120, in <module>\n",
            "    print(i, keyness[i][0])\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "- How do text dispersion keywords differ from the standard ones? Or do they?\n",
        "- And again: is the keyword quality better for some registers? What happens if you try with lemmas?"
      ],
      "metadata": {
        "id": "gM0AHQYkVOf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering\n",
        "So far, we have not used the benefits of the conllu format w.r.t the filtering of \"uninformative\" parts-of-speech.\n",
        "\n",
        "However, this can be useful (depending on the keyness method we use). \n",
        "\n",
        "In the script below you can indicate the POS classes you want to exclude. The pos tags used in the conll format are:\n",
        "```\n",
        "ADJ: adjective\n",
        "ADP: adposition\n",
        "ADV: adverb\n",
        "AUX: auxiliary\n",
        "CCONJ: coordinating conjunction\n",
        "DET: determiner\n",
        "INTJ: interjection\n",
        "NOUN: noun\n",
        "NUM: numeral\n",
        "PART: particle\n",
        "PRON: pronoun\n",
        "PROPN: proper noun\n",
        "PUNCT: punctuation\n",
        "SCONJ: subordinating conjunction\n",
        "SYM: symbol\n",
        "VERB: verb\n",
        "X: other\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lOAfRRvKWzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The keyness scripts using filtering need a stop word file where you list all the POS classes that you want to filter out."
      ],
      "metadata": {
        "id": "0CwNEgxbkEmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"ADJ PUNCT DET PRON PART\" > stop.txt # echo prints the tags to the file\n",
        "! cat stop.txt # cat prints the file here so we can have a look at it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSzyLYeLrN_1",
        "outputId": "96e97546-b29e-4658-dc4b-dd7e632154d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADJ PUNCT DET PRON PART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try out the filtered keyness scripts below. \n",
        "\n",
        "Both keyness scripts take `stop.txt` as an argument, as you can notice from the command"
      ],
      "metadata": {
        "id": "oZFtokc8x13W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 keyness-filt.py data/sr.conllu data/df.conllu \"FORM\" stop.txt | head -40"
      ],
      "metadata": {
        "id": "iyprwGyTrWHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0c9f86-19db-4cb6-e0d0-0f7bdf3aeba6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Input_Corpus ...\n",
            "done!\n",
            "Reading Reference Corpus ...\n",
            "done!\n",
            "--------------------------------------------------\n",
            "Corpus name Input_Corpus\n",
            "Keyword, keyness value\n",
            "team 1444.0\n",
            "season 1170.56\n",
            "league 732.09\n",
            "players 673.03\n",
            "game 656.1\n",
            "win 579.37\n",
            "against 492.42\n",
            "three 473.71\n",
            "play 467.42\n",
            "race 456.44\n",
            "in 450.38\n",
            "games 416.19\n",
            "ball 412.25\n",
            "points 411.85\n",
            "football 404.32\n",
            "cup 391.8\n",
            "coach 374.38\n",
            "two 358.69\n",
            "player 357.01\n",
            "four 355.15\n",
            "teams 345.27\n",
            "vettel 344.39\n",
            "goal 337.55\n",
            "won 330.59\n",
            "five 319.53\n",
            "has 306.25\n",
            "arsenal 300.5\n",
            "wenger 299.84\n",
            "united 298.24\n",
            "goals 295.89\n",
            "said 293.09\n",
            "club 289.9\n",
            "year 288.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 text_dispersion_filt.py data/sr.conllu data/df.conllu FORM stop.txt | head -40"
      ],
      "metadata": {
        "id": "kGgH6DuCxU-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d64de4-0e2b-4dd1-973f-11633dba74b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos data/df.conllu\n",
            "Reading a conllu file named data/df.conllu\n",
            "file match data/df.conllu\n",
            "pos data/sr.conllu\n",
            "Reading a conllu file named data/sr.conllu\n",
            "file match data/sr.conllu\n",
            "\n",
            "Text dispersion keywords for data/sr.conllu\n",
            "season 278.63649189897933\n",
            "team 249.8567972281745\n",
            "league 209.3395896410617\n",
            "win 194.15854527065773\n",
            "coach 182.90443514605633\n",
            "game 164.5988062685827\n",
            "three 163.6876659620223\n",
            "players 161.96862063145053\n",
            "five 159.88954214316306\n",
            "four 149.69814763788727\n",
            "won 147.4713323782174\n",
            "teams 143.81480788711949\n",
            "victory 142.96050040128654\n",
            "games 140.70458967799044\n",
            "against 136.05173611466475\n",
            "scoring 134.52289929812304\n",
            "play 133.02157653240735\n",
            "saturday 130.80478903072807\n",
            "winning 123.98408080064502\n",
            "football 122.63054198772535\n",
            "scored 120.39621800857634\n",
            "cup 118.81271430035525\n",
            "injury 117.51784083340803\n",
            "player 114.7401192420264\n",
            "played 113.65815584424207\n",
            "champions 113.55789009126187\n",
            "seven 112.62354106263122\n",
            "six 111.62604043035991\n",
            "ball 110.1619835408664\n",
            "goal 107.52470002995996\n",
            "points 101.96761693240062\n",
            "eight 101.84144776172519\n",
            "Traceback (most recent call last):\n",
            "  File \"text_dispersion_filt.py\", line 122, in <module>\n",
            "    print(i, keyness[i][0])\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "Do the results become better when you use filtering? \n",
        "\n",
        "Are there differences between the methods?\n",
        "\n",
        "What POS classes would you see as the best to use as stop?\n",
        "\n",
        "Do you see any risks in using such stop lists?\n",
        "\n",
        "What kinds of research questions could you use these methods to?"
      ],
      "metadata": {
        "id": "ECGTe0cAzBle"
      }
    }
  ]
}