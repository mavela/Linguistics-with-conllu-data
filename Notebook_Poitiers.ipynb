{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-Poitiers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS+XkvcXnDI4krjNyIH610",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavela/Linguistics-with-conllu-data/blob/master/Notebook_Poitiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Foreword\n",
        "\n",
        "Focus here on ready-made Python scripts\n",
        "\n",
        "(Although some of the first commands are in Bash)\n",
        "\n",
        "Bash commands need to begin with` ! `here\n",
        "\n",
        "All the commands can be run by clicking the arrow\n",
        "\n",
        "You can also change input files and variables as instructed below\n",
        "Focus here on ready-made Python scripts\n",
        "\n",
        "**Questions** sections point you to possibly interesting things\n",
        "\n",
        "All the scripts are downloadable at https://github.com/mavela/Linguistics-with-conllu-data.git (disclaimer for code beauty!)"
      ],
      "metadata": {
        "id": "JzWDcp3FOdXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "\n",
        "#Preparations\n",
        "Let's download the data from Github - this is done using `git clone + repo address`\n",
        "\n",
        "`Git clones` the entire folder to the colab directory\n",
        "\n",
        "`% cd` takes us to the correct directory\n",
        "\n",
        "`! ls` lists the files in that directory"
      ],
      "metadata": {
        "id": "egn0EwFPO4DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/mavela/Linguistics-with-conllu-data.git"
      ],
      "metadata": {
        "id": "YSZPYN8lPcWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "% cd Linguistics-with-conllu-data/\n",
        "! ls"
      ],
      "metadata": {
        "id": "lCHrNsoi4AKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Here is the data folder\"\n",
        "! ls data/\n"
      ],
      "metadata": {
        "id": "zicJiMey4UNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then let's have a look at the data\n",
        "\n",
        "The files are from the Corpus of Online Register of English (Biber & Egbert 2018)\n",
        "\n",
        "df = discussion forum\n",
        "\n",
        "ds = description with intent to sell\n",
        "\n",
        "sr = sports report\n",
        "\n",
        "ht = how-to\n",
        "\n",
        "You can print N first lines of the file with `head -N`\n",
        "\n",
        "### Questions: \n",
        "\n",
        "- You can try to print different files!"
      ],
      "metadata": {
        "id": "jYbqyUN7QLXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bpqVTNqG0Cc"
      },
      "outputs": [],
      "source": [
        "! head -20 data/sr.conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hoq many tokens?\n",
        "### Questions\n",
        "\n",
        "- How many tokens?\n",
        "- Are there differences between the datasets?"
      ],
      "metadata": {
        "id": "-Qsn46_IR10_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import count_words, most_frequent, extract_register, print_text\n",
        "\n",
        "print(\"Total word count of the conllu file is\", count_words(\"data/sr.conllu\"), \"tokens\")"
      ],
      "metadata": {
        "id": "hidsAEzvR_a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print words, lemmas or other things\n",
        "\n",
        "`print_text` prints one text per line the text featurized as we indicate.\n",
        "\n",
        "We can specify the kind of feature by referring to the column name in the Conll format.\n",
        "\n",
        "The columns are:\n",
        "\n",
        "`ID, FORM, LEMMA, UPOS, XPOS, FEAT, HEAD, DEPREL, DEPS, MISC`\n",
        "\n",
        "The numeric argument indicates how many texts we want.\n",
        "\n",
        "### Questions\n",
        "You can try how the texts look like when featurized. How would you estimate the parser performance?"
      ],
      "metadata": {
        "id": "feb-zWV7TQIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_text(\"data/df.conllu\", \"FORM\", 2))\n",
        "print()\n",
        "## Then try with some other text class and feature!"
      ],
      "metadata": {
        "id": "5jgFn-YWTSGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "\n",
        "##Type A perspective: Analyzing / searching for individual words / lemmas / something\n",
        "\n",
        "### Questions\n",
        "\n",
        "How many times a particular lemma appears in the corpus?\n",
        "\n",
        "What are its most frequent dependency relations? (Or other tags - you can change the DEPREL / FORM part and the conllu file!)"
      ],
      "metadata": {
        "id": "jea8SURaVhjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import count_specific_lemma, count_word_context\n",
        "\n",
        "print(count_specific_lemma(\"you\", \"data/sr.conllu\", \"FORM\"))\n",
        "print(count_specific_lemma(\"think\", \"data/sr.conllu\", \"DEPREL\"))"
      ],
      "metadata": {
        "id": "QpO9qD9Z28ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question\n",
        "How does a word appear in context? "
      ],
      "metadata": {
        "id": "v64_XHOSJ6WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from analyze import read_conllu\n",
        "ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC=range(10)\n",
        "\n",
        "count = 0\n",
        "for comm, sent in read_conllu(open(\"data/sr.conllu\", \"r\")): # here you can change the data file\n",
        "    if \"nsubj\" in [token[DEPREL].lower() for token in sent]: #here you can put any word form instead of \"you\"\n",
        "        count += 1\n",
        "        if count > 5: # here we specify how many sentences we want to see\n",
        "          break\n",
        "        else:\n",
        "          print(\" \".join(token[FORM] for token in sent)) # now this prints the sentence FORMs and DEPRELs, but these can be changed as well\n",
        "          print(\" \".join(token[DEPREL] for token in sent))\n",
        "          print()\n"
      ],
      "metadata": {
        "id": "dTbK2Qn_UY33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "\n",
        "## Type B perspective: Text level\n",
        "Most frequent tokens + lemmas in a text"
      ],
      "metadata": {
        "id": "GuM3EzBo3atO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent tokens\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"FORM\", 10)) # the number defines how many we want to see\n",
        "\n",
        "print(\"Most frequent lemmas\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"LEMMA\", 10)) # the number defines how many we want to see"
      ],
      "metadata": {
        "id": "IxqkHsbc3kF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of POS tags or dependency relations\n",
        "\n",
        "The Conllu tagsets (columns) are defined as: `ID,FORM,LEMMA,UPOS,XPOS,FEAT,HEAD,DEPREL,DEPS,MISC`"
      ],
      "metadata": {
        "id": "c-xnGF5734pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent frequent part-of-speech tags\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"UPOS\", 10))\n",
        "\n",
        "print(\"Then the most frequent dependency relations\")\n",
        "print(most_frequent(\"data/sr.conllu\", \"DEPREL\", 10))\n"
      ],
      "metadata": {
        "id": "XnBARAuM3787"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question:\n",
        "- Can you see any differences in the most frequent words between the files? Can you think of any reasons for the similarities / differences you see?"
      ],
      "metadata": {
        "id": "VbGtGQPp6QQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focusing text-level analysis to particular tags\n",
        "\n",
        "### Questions\n",
        "\n",
        "What are the most frequent adjectives / verbs / etc. in the dataset?\n",
        "\n",
        "Do these differ between the datasets?"
      ],
      "metadata": {
        "id": "7BH9q1Q14LEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most frequent lemmas under a specific tagset(column).\")\n",
        "print(\"For instance, the most frequent lemmas that receive the ADJ tag in the UPOS column.\")\n",
        "print()\n",
        "print(most_frequent(\"data/sr.conllu\", \"UPOS\", 10, \"ADJ\"))\n",
        "\n",
        "print(\"Or the most frequent lemmas that receive nsubj that in the DEPREL column\")\n",
        "print()\n",
        "print(most_frequent(\"data/sr.conllu\", \"DEPREL\", 10, \"nsubj\"))\n"
      ],
      "metadata": {
        "id": "krqqnwgK4J_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyness\n",
        "\n",
        "### Standard target+reference corpus comparison with log likelihood\n",
        "\n",
        "`\"FORM\"` specifies the conlllu format column you want to focus, so you can use `\"LEMMA\"` too\n",
        "\n",
        "Note that we call the keyness script from bash here.\n",
        "\n",
        "The keywords are counted for `ds.conllu` in comparison with `ht.conllu`\n",
        "\n",
        "Remember that you can change these!"
      ],
      "metadata": {
        "id": "SYfRXXYl4eNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 keyness-conllu.py data/ds.conllu data/ht.conllu \"FORM\" | head -30 \n"
      ],
      "metadata": {
        "id": "2h-XUOxWGD9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions: \n",
        "- What happens if you try to get keywords using lemmas? \n",
        "- Do all the registers get good keywords, or are some registers described better? Do you get descriptive keywords for all the registers?\n",
        "- Does changing the reference corpus change the results?\n",
        "\n",
        "### Note!\n",
        "An essential part of any linguistic analysis is to understand what the numerically extracted results (here the keywords) mean in context. To this end, you  can print texts with the keywords using the `read_conllu.py` above."
      ],
      "metadata": {
        "id": "_54quPa5Gfe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text dispersion keyness\n",
        "\n",
        "Counted the same way as the standard keyness above"
      ],
      "metadata": {
        "id": "Tpv9i-0A_fFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 text_dispersion.py data/sr.conllu data/df.conllu \"FORM\" | head -40"
      ],
      "metadata": {
        "id": "4AHOEtJ5T3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "- How do text dispersion keywords differ from the standard ones? Or do they?\n",
        "- And again: is the keyword quality better for some registers? What happens if you try with lemmas?"
      ],
      "metadata": {
        "id": "gM0AHQYkVOf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering\n",
        "So far, we have not used the benefits of the conllu format w.r.t the filtering of \"uninformative\" parts-of-speech.\n",
        "\n",
        "However, this can be useful (depending on the keyness method we use). \n",
        "\n",
        "In the script below you can indicate the POS classes you want to exclude. The pos tags used in the conll format are:\n",
        "```\n",
        "ADJ: adjective\n",
        "ADP: adposition\n",
        "ADV: adverb\n",
        "AUX: auxiliary\n",
        "CCONJ: coordinating conjunction\n",
        "DET: determiner\n",
        "INTJ: interjection\n",
        "NOUN: noun\n",
        "NUM: numeral\n",
        "PART: particle\n",
        "PRON: pronoun\n",
        "PROPN: proper noun\n",
        "PUNCT: punctuation\n",
        "SCONJ: subordinating conjunction\n",
        "SYM: symbol\n",
        "VERB: verb\n",
        "X: other\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lOAfRRvKWzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The keyness scripts using filtering need a stop word file where you list all the POS classes that you want to filter out."
      ],
      "metadata": {
        "id": "0CwNEgxbkEmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"ADJ PUNCT DET PRON PART\" > stop.txt # echo prints the tags to the file\n",
        "! cat stop.txt # cat prints the file here so we can have a look at it"
      ],
      "metadata": {
        "id": "dSzyLYeLrN_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try out the filtered keyness scripts below. \n",
        "\n",
        "Both keyness scripts take `stop.txt` as an argument, as you can notice from the command"
      ],
      "metadata": {
        "id": "oZFtokc8x13W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 keyness-filt.py data/sr.conllu data/df.conllu \"FORM\" stop.txt | head -40"
      ],
      "metadata": {
        "id": "iyprwGyTrWHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 text_dispersion_filt.py data/sr.conllu data/df.conllu FORM stop.txt | head -40"
      ],
      "metadata": {
        "id": "kGgH6DuCxU-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "Do the results become better when you use filtering? \n",
        "\n",
        "Are there differences between the methods?\n",
        "\n",
        "What POS classes would you see as the best to use as stop?\n",
        "\n",
        "Do you see any risks in using such stop lists?\n",
        "\n",
        "What kinds of research questions could you use these methods to?"
      ],
      "metadata": {
        "id": "ECGTe0cAzBle"
      }
    }
  ]
}